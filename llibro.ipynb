{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hoja de trabajo 2\n",
    "Integrantes:\n",
    "- Juan Pablo Solis\n",
    "- Brandon Reyes\n",
    "- Carlos Valladares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Responda a cada de las siguientes preguntas de forma clara y lo más completamente posible.\n",
    "1. Defina el proceso de decisión de Markov (MDP) y explique sus componentes.\n",
    "\n",
    "    Un MDP (Proceso de Decisión de Markov) es un marco matemático para modelar problemas de toma de decisiones en un entorno incierto. Se emplea en inteligencia artificial para la representación de decisiones secuenciales por ende el resultado de una acción depende del estado actual y de una función de probabilidad.\n",
    "    Los componentes son:\n",
    "     - Estados (S): Conjunto de posibles situaciones en las que puede estar el agente.\n",
    "     - Acciones (A): Conjunto de acciones disponibles en cada estado.\n",
    "     - Matriz de transición (T): Probabilidad de pasar de un estado a otro dado que se toma una acción.\n",
    "     - Recompensa (R): Valor que indica qué tan buena es una acción en un estado determinado.\n",
    "     - Factor de descuento (γ - gamma): Indica la importancia de las recompensas futuras.\n",
    "\n",
    "\n",
    "2. Describa cual es la diferencia entre política, evaluación de políticas, mejora de políticas e iteración de políticas\n",
    "     - Política (π): Es una función que indica qué acción debe tomar el agente en cada estado. Básicamente, es una estrategia de decisión.\n",
    "     - Evaluación de políticas: Se refiere al proceso de calcular cuán buena es una política, es decir, el valor esperado de seguir esa política en cada estado.\n",
    "     - Mejora de políticas: Implica modificar la política actual con base en la evaluación de políticas, buscando obtener mejores recompensas.\n",
    "     - Iteración de políticas: Es un proceso iterativo que alterna entre evaluación y mejora de políticas hasta encontrar la mejor posible.\n",
    "\n",
    "\n",
    "3. Explique el concepto de factor de descuento (gamma) en los MDP. ¿Cómo influye en la toma de decisiones?\n",
    "\n",
    "    El factor de descuento con el simbolo (γ) es un número entre 0 y 1 que controla la importancia de las recompensas futuras en la toma de decisiones.\n",
    "     - Si gamma (γ) es cercano a 0, el agente se enfoca solo en las recompensas inmediatas y no se preocupa por el futuro.\n",
    "     - Si gamma (γ) es cercano a 1, el agente da más peso a las recompensas futuras y planifica mejor sus acciones.\n",
    "\n",
    "4. Analice la diferencia entre los algoritmos de iteración de valores y de iteración de políticas para resolver MDP.\n",
    "\n",
    "    Ambas técnicas se usan para encontrar la mejor política en un MDP, pero funcionan de manera diferente:\n",
    "\n",
    "    Iteración de Valores (Value Iteration):\n",
    "\n",
    "    - Se basa en calcular directamente el valor de cada estado.\n",
    "    - No necesita definir una política al inicio, sino que ajusta los valores hasta que se estabilicen.\n",
    "    - Es útil cuando queremos encontrar los valores óptimos sin probar políticas específicas.\n",
    "\n",
    "    Iteración de Políticas (Policy Iteration):\n",
    "\n",
    "    - Se parte de una política inicial y se mide su rendimiento.\n",
    "    - Luego, la política se ajusta para mejorar los resultados.\n",
    "    - Alterna entre evaluación y mejora hasta que la política converge a la mejor posible.\n",
    "\n",
    "\n",
    "5. ¿Cuáles son algunos desafíos o limitaciones comunes asociados con la resolución de MDP a gran escala?\n",
    "Discuta los enfoques potenciales para abordar estos desafíos\n",
    "\n",
    "    La resolución de Procesos de Decisión de Markov (MDP) a gran escala enfrenta varios desafíos, siendo uno de los más importantes la explosión del espacio de estados, ya que el número de estados posibles puede crecer exponencialmente con el tamaño del problema, dificultando su almacenamiento y procesamiento. Además, el costo computacional para calcular políticas óptimas puede ser elevado, especialmente en entornos con muchas acciones y transiciones posibles. Otro problema frecuente es la incertidumbre en la información, ya que en muchas situaciones reales no se conocen con precisión las probabilidades de transición ni las recompensas esperadas. Para abordar estos desafíos, se pueden emplear estrategias como la reducción del espacio de estados, mediante técnicas de agregación de estados o funciones de aproximación. También, el uso de aprendizaje por refuerzo, con algoritmos como Q-Learning o Deep Q Networks (DQN), permite que los agentes aprendan políticas óptimas sin necesidad de modelar todo el sistema. Finalmente, la aplicación de heurísticas y métodos de estimación ayuda a reducir la complejidad computacional, facilitando la toma de decisiones en entornos dinámicos y de gran escala.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Task 2 - Preguntas Analiticas**\n",
    "\n",
    "- **Analice críticamente los supuestos subyacentes a la propiedad de Markov en los Procesos de Decisión de Markov (MDP). Analice escenarios en los que estos supuestos puedan no ser válidos y sus implicaciones para la toma de decisiones.¨**\n",
    "- Los Procesos de Decisión de Markov son un marco matemático para modelar la toma de decisiones en entornos estocásticos. La propiedad de Markov establece que la distribución de probabilidad del estado futuro depende únicamente del estado actual y no del historial de estados previos. Es decir, el futuro es independiente del pasado dado el presente. Esta tiene 3 supuestos clave siendo:  La transición de un estado a otro solo depende del estado actual y la acción tomada, no de la historia previa de estados y acciones, se asume que la probabilidad de moverse de un estado a otro es constante en el tiempo y la función de recompensa solo depende del estado actual y la acción elegida\n",
    "    - Existen distintos escenarios donde Markov puede llegar a no ser valido, el primer caso es en sistemas en donde el estado actual no encapsula toda la información relevante para la toma de decisiones. Un ejemplo de esto es en finanzas, la tendencia pasada de un activo financiero puede influir en su valor futuro, lo que contradice la propiedad de Markov. \n",
    "    - Un segundo caso son en problemas donde la historia completa afecta la evolución del sistema, la suposición de independencia del pasado falla. Un ejemplo de esto seria En diagnósticos médico, en donde un paciente con historial de enfermedades crónicas puede tener una progresión de enfermedad que no puede representarse únicamente con el estado actual.\n",
    "    - Otro caso seria en entornos dinámicos, las probabilidades de transición pueden cambiar con el tiempo, por ejemplo en los mercados económicos, la probabilidad de que una empresa cambie de estado financiero varía en función de crisis económicas, regulaciones o cambios en el mercado\n",
    "    - Por ultimo seria en casos donde las recompensas acumuladas afectan decisiones futuras, la formulación estándar de MDP se vuelve inadecuada, tal como en juegos de rol donde las decisiones pasadas de un jugador pueden desbloquear o restringir ciertas recompensas futuras\n",
    "- Las implicaciones de estas violaciones a la propiedad de Markov son significativas en la toma de decisiones. Si se utiliza un MDP en un entorno donde la historia pasada es relevante, las decisiones calculadas pueden ser subóptimas o incluso perjudiciales. En estos casos, es necesario recurrir a modelos más avanzados, como los Procesos de Decisión de Markov Parcialmente Observables, que permiten capturar información oculta, o técnicas de aprendizaje profundo con redes neuronales recurrentes, que pueden modelar dependencias temporales. Sin embargo, estos enfoques suelen ser computacionalmente más costosos y complejos de implementar.\n",
    "\n",
    "- **Explore los desafíos de modelar la incertidumbre en los procesos de decisión de Markov (MDP) y analice estrategias para una toma de decisiones sólida en entornos inciertos**\n",
    "-   Existen diversos desafios al momento de modelar Proceso de Markov pero uno de los retos más grandes es al momento de definir la incertidumbre, esto principalmente debido a que su efectividad depende de la precisión con la que se modela la incertidumbre en la dinámica del sistema y en la función de recompensas. La incertidumbre puede surgir de varias fuentes, lo que representa desafíos significativos en la formulación y solución de los MDP. Uno de los principales desafíos es la incertidumbre en las probabilidades de transición. En muchos escenarios reales, las transiciones entre estados no son completamente conocidas o pueden cambiar con el tiempo. Por ejemplo, en la gestión de inventarios, la demanda de productos puede variar de manera impredecible, haciendo difícil definir una matriz de transición precisa. Un modelo que asuma probabilidades erróneas podría conducir a políticas subóptimas. Otro reto importante es la incertidumbre en las recompensas. En algunos entornos, las recompensas pueden depender de factores externos difíciles de modelar o predecir. Por ejemplo, en la planificación de rutas para vehículos autónomos, las condiciones del tráfico pueden influir en la eficiencia del recorrido, lo que hace que la recompensa de una acción específica no sea siempre la esperada. Si la función de recompensa está mal definida, las decisiones optimizadas pueden no ser las mejores en la práctica. Además, la incertidumbre en la observación del estado representa un desafío cuando el estado real del sistema no es completamente observable. En estos casos, los MDP estándar no son adecuados, y se requieren enfoques como los Procesos de Decisión de Markov Parcialmente Observables, que permiten tomar decisiones con información incompleta. \n",
    "- Para poder vencer estos desafios y mejorar la toma de decisiones del programa existen diversas estrategias las cuales se pueden aplicar: La primera estrategia es la estimación adaptativa de las probabilidades de transición. En lugar de asumir probabilidades fijas, se pueden emplear enfoques de aprendizaje, como el aprendizaje por refuerzo, para ajustar dinámicamente las probabilidades de transición en función de la experiencia obtenida. Esto permite que el modelo se adapte a cambios en el entorno sin requerir una redefinición manual. Otra estrategia  es el uso de métodos robustos para la optimización de políticas. En lugar de asumir que las recompensas y transiciones son exactamente conocidas, se pueden emplear enfoques de optimización robusta que consideran incertidumbre en sus parámetros. Esto permite generar políticas que funcionan bien en una variedad de escenarios posibles, en lugar de estar ajustadas únicamente a estimaciones específicas. Por ultimo, el uso de simulación y métodos Monte Carlo puede ser útil para evaluar la robustez de las políticas antes de implementarlas en el mundo real. Mediante simulaciones de diferentes escenarios posibles, es posible analizar el impacto de la incertidumbre y ajustar las estrategias de toma de decisiones para minimizar riesgos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3 - Preguntas Prácticas\n",
    "Desarrolle un agente básico capaz de resolver un problema simplificado del Proceso de Decisión de Markov (MDP).\n",
    "Considere utilizar un ejemplo bien conocido como el entorno 'Frozen Lake'. Proporcione el código Python para el\n",
    "proceso de toma de decisiones del agente basándose únicamente en los principios de los procesos de decisión de\n",
    "Markov. Recuerde que para este tipo de problema, el ambiente es una matriz de 4x4, y las acciones, pueden ser\n",
    "moverse hacia arriba, abajo, derecha, izquierda. Considere que el punto inicial siempre estará en la esquina opuesta\n",
    "del punto de meta. Es decir, puede tener hasta 4 configuraciones diferentes. Por ejemplo, el punto inicial puede estar\n",
    "en la coordenada (0, 0) y el punto de meta en la coordenada en la coordenada (3, 3). Además, la posición de los hoyos\n",
    "debe ser determinada aleatoriamente y no debe superar el ser más de 3. Es decir, si aleatoriamente se decide que\n",
    "sean 2 posiciones de hoyo, las coordenadas de estas deben ser determinadas de forma aleatoria. Asegúrese de usar\n",
    "“seed” para que sus resultados sean consistentes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parámetros del entorno\n",
    "dim = 4  # Tamaño del lago (4x4)\n",
    "GAMMA = 0.9  # Factor de descuento\n",
    "THETA = 1e-3  # Umbral de convergencia\n",
    "ACCIONES = ['Subir', 'Bajar', 'Izquierda', 'Derecha']\n",
    "MOVES = {\n",
    "    'Subir': (-1, 0), 'Bajar': (1, 0),\n",
    "    'Izquierda': (0, -1), 'Derecha': (0, 1)\n",
    "}\n",
    "\n",
    "# Semilla para reproducibilidad\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generar posiciones de inicio y meta\n",
    "start_pos = (0, 0)\n",
    "goal_pos = (dim - 1, dim - 1)\n",
    "\n",
    "# Generar posiciones de hoyos aleatorios\n",
    "num_holes = np.random.randint(1, 4)  # Máximo 3 hoyos\n",
    "hole_positions = set()\n",
    "while len(hole_positions) < num_holes:\n",
    "    pos = (np.random.randint(0, dim), np.random.randint(0, dim))\n",
    "    if pos not in [start_pos, goal_pos]:\n",
    "        hole_positions.add(pos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifica si una posición está dentro de los límites y no es un hoyo\n",
    "def is_valid(pos):\n",
    "    return 0 <= pos[0] < dim and 0 <= pos[1] < dim and pos not in hole_positions\n",
    "\n",
    "\n",
    "# Devuelve la distribución de probabilidades de los estados siguientes\n",
    "def get_transition_probs(estado, ACCION):\n",
    "    new_pos = (estado[0] + MOVES[ACCION][0], estado[1] + MOVES[ACCION][1])\n",
    "    if not is_valid(new_pos):\n",
    "        return {estado: 1.0}  # Si choca, quedarse en el mismo lugar\n",
    "    return {new_pos: 1.0}\n",
    "\n",
    "\n",
    "# Aplica iteración de valores para encontrar la mejor política  \n",
    "def value_iteration():\n",
    "    V = np.zeros((dim, dim))  # Inicializar valores en 0\n",
    "    policy = np.full((dim, dim), '', dtype=object)\n",
    "    \n",
    "    while True:\n",
    "        delta = 0\n",
    "        for i in range(dim):\n",
    "            for j in range(dim):\n",
    "                estado = (i, j)\n",
    "                if estado in hole_positions or estado == goal_pos:\n",
    "                    continue  # Saltar hoyos y meta\n",
    "                \n",
    "                best_value = float('-inf')\n",
    "                best_ACCION = None\n",
    "                \n",
    "                for ACCION in ACCIONES:\n",
    "                    next_estados = get_transition_probs(estado, ACCION)\n",
    "                    value = sum(prob * (1 + GAMMA * V[next_estado]) for next_estado, prob in next_estados.items())\n",
    "                    \n",
    "                    if value > best_value:\n",
    "                        best_value = value\n",
    "                        best_ACCION = ACCION\n",
    "                \n",
    "                delta = max(delta, abs(V[i, j] - best_value))\n",
    "                V[i, j] = best_value\n",
    "                policy[i, j] = best_ACCION\n",
    "        \n",
    "        if delta < THETA:\n",
    "            break\n",
    "    \n",
    "    return policy\n",
    "\n",
    "\n",
    "# Muestra la cuadrícula del entorno con la política encontrada\n",
    "def print_grid(policy):\n",
    "    for i in range(dim):\n",
    "        row = ''\n",
    "        for j in range(dim):\n",
    "            estado = (i, j)\n",
    "            if estado == start_pos:\n",
    "                row += 'S '\n",
    "            elif estado == goal_pos:\n",
    "                row += 'G '\n",
    "            elif estado in hole_positions:\n",
    "                row += 'X '\n",
    "            else:\n",
    "                row += policy[i, j][0] + ' '  # Solo la primera letra\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Política óptima encontrada:\n",
      "S B X B \n",
      "S S I S \n",
      "S S X S \n",
      "X S I G \n"
     ]
    }
   ],
   "source": [
    "# Ejecutar iteración de valores\n",
    "policy = value_iteration()\n",
    "print(\"Política óptima encontrada:\")\n",
    "print_grid(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'Subir' → S\n",
    "\n",
    "'Bajar' → B\n",
    "\n",
    "'Izquierda' → I\n",
    "\n",
    "'Derecha' → D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Referencias:\n",
    " -  Sucar, L. E. (2018). Procesos de decisión de Markov y aprendizaje por refuerzo. En Inteligencia Artificial: Un Enfoque Práctico (pp. 345-370). Instituto Nacional de Astrofísica, Óptica y Electrónica. Recuperado de https://ccc.inaoep.mx/~emorales/Papers/2018/2018ESucar.pdf\n",
    " - Puterman, M. L. (1994). Markov Decision Processes: Discrete Stochastic Dynamic Programming. John Wiley & Sons.\n",
    " - Sutton, R. S., & Barto, A. G. (2018). Reinforcement Learning: An Introduction (2nd Edition). MIT Press.\n",
    " - Russell, S., & Norvig, P. (2020). Artificial Intelligence: A Modern Approach (4th Edition). Pearson.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
