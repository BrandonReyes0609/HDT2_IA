{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hoja de trabajo 2\n",
    "Integrantes:\n",
    "- Juan Pablo Solis\n",
    "- Brandon Reyes\n",
    "- Carlos Valladares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Responda a cada de las siguientes preguntas de forma clara y lo más completamente posible.\n",
    "1. Defina el proceso de decisión de Markov (MDP) y explique sus componentes.\n",
    "\n",
    "    Un MDP (Proceso de Decisión de Markov) es un marco matemático para modelar problemas de toma de decisiones en un entorno incierto. Se emplea en inteligencia artificial para la representación de decisiones secuenciales por ende el resultado de una acción depende del estado actual y de una función de probabilidad.\n",
    "    Los componentes son:\n",
    "     - Estados (S): Conjunto de posibles situaciones en las que puede estar el agente.\n",
    "     - Acciones (A): Conjunto de acciones disponibles en cada estado.\n",
    "     - Matriz de transición (T): Probabilidad de pasar de un estado a otro dado que se toma una acción.\n",
    "     - Recompensa (R): Valor que indica qué tan buena es una acción en un estado determinado.\n",
    "     - Factor de descuento (γ - gamma): Indica la importancia de las recompensas futuras.\n",
    "\n",
    "\n",
    "2. Describa cual es la diferencia entre política, evaluación de políticas, mejora de políticas e iteración de políticas\n",
    "     - Política (π): Es una función que indica qué acción debe tomar el agente en cada estado. Básicamente, es una estrategia de decisión.\n",
    "     - Evaluación de políticas: Se refiere al proceso de calcular cuán buena es una política, es decir, el valor esperado de seguir esa política en cada estado.\n",
    "     - Mejora de políticas: Implica modificar la política actual con base en la evaluación de políticas, buscando obtener mejores recompensas.\n",
    "     - Iteración de políticas: Es un proceso iterativo que alterna entre evaluación y mejora de políticas hasta encontrar la mejor posible.\n",
    "\n",
    "\n",
    "3. Explique el concepto de factor de descuento (gamma) en los MDP. ¿Cómo influye en la toma de decisiones?\n",
    "\n",
    "    El factor de descuento con el simbolo (γ) es un número entre 0 y 1 que controla la importancia de las recompensas futuras en la toma de decisiones.\n",
    "     - Si gamma (γ) es cercano a 0, el agente se enfoca solo en las recompensas inmediatas y no se preocupa por el futuro.\n",
    "     - Si gamma (γ) es cercano a 1, el agente da más peso a las recompensas futuras y planifica mejor sus acciones.\n",
    "\n",
    "4. Analice la diferencia entre los algoritmos de iteración de valores y de iteración de políticas para resolver MDP.\n",
    "\n",
    "    Ambas técnicas se usan para encontrar la mejor política en un MDP, pero funcionan de manera diferente:\n",
    "\n",
    "    Iteración de Valores (Value Iteration):\n",
    "\n",
    "    - Se basa en calcular directamente el valor de cada estado.\n",
    "    - No necesita definir una política al inicio, sino que ajusta los valores hasta que se estabilicen.\n",
    "    - Es útil cuando queremos encontrar los valores óptimos sin probar políticas específicas.\n",
    "\n",
    "    Iteración de Políticas (Policy Iteration):\n",
    "\n",
    "    - Se parte de una política inicial y se mide su rendimiento.\n",
    "    - Luego, la política se ajusta para mejorar los resultados.\n",
    "    - Alterna entre evaluación y mejora hasta que la política converge a la mejor posible.\n",
    "\n",
    "\n",
    "5. ¿Cuáles son algunos desafíos o limitaciones comunes asociados con la resolución de MDP a gran escala?\n",
    "Discuta los enfoques potenciales para abordar estos desafíos\n",
    "\n",
    "    La resolución de Procesos de Decisión de Markov (MDP) a gran escala enfrenta varios desafíos, siendo uno de los más importantes la explosión del espacio de estados, ya que el número de estados posibles puede crecer exponencialmente con el tamaño del problema, dificultando su almacenamiento y procesamiento. Además, el costo computacional para calcular políticas óptimas puede ser elevado, especialmente en entornos con muchas acciones y transiciones posibles. Otro problema frecuente es la incertidumbre en la información, ya que en muchas situaciones reales no se conocen con precisión las probabilidades de transición ni las recompensas esperadas. Para abordar estos desafíos, se pueden emplear estrategias como la reducción del espacio de estados, mediante técnicas de agregación de estados o funciones de aproximación. También, el uso de aprendizaje por refuerzo, con algoritmos como Q-Learning o Deep Q Networks (DQN), permite que los agentes aprendan políticas óptimas sin necesidad de modelar todo el sistema. Finalmente, la aplicación de heurísticas y métodos de estimación ayuda a reducir la complejidad computacional, facilitando la toma de decisiones en entornos dinámicos y de gran escala.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Referencias:\n",
    " -  Sucar, L. E. (2018). Procesos de decisión de Markov y aprendizaje por refuerzo. En Inteligencia Artificial: Un Enfoque Práctico (pp. 345-370). Instituto Nacional de Astrofísica, Óptica y Electrónica. Recuperado de https://ccc.inaoep.mx/~emorales/Papers/2018/2018ESucar.pdf\n",
    " - Sutton, R. S., & Barto, A. G. (2018). Reinforcement Learning: An Introduction (2nd Edition). MIT Press.\n",
    " - Russell, S., & Norvig, P. (2020). Artificial Intelligence: A Modern Approach (4th Edition). Pearson.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
